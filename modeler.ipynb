{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bitenv3206f7551eaf4ab3b1d866b11f23a5c8",
   "display_name": "Python 3.7.0 64-bit ('env')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Topic Modeler\n",
    "Discovering abstract topics in documents (Flipgrid Topics). Using Latent Dirichlet Allocation (LDA)we will look for relevant collections of words, or topics, in the corpus.\n",
    "\n",
    "#### Packages\n",
    "* spaCy (NLP)\n",
    "* Gensim (Topic modeling library)\n",
    "* Numpy (Math stuff)\n",
    "* Pandas (Data analysis)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "source": [
    "## Cleaning the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n",
    "\n",
    "doc = nlp('''Mike Tyson was a great boxer. He was a boxer who won many belts.''')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "doc_list.append(doc)\n",
    "words = corpora.Dictionary(doc_list)\n",
    "\n",
    "# Turns each document into a bag of words.\n",
    "bow = [words.doc2bow(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=bow,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0,\n  '0.167*\"boxer\" + 0.167*\"win\" + 0.167*\"Mike\" + 0.167*\"Tyson\" + 0.167*\"belt\" + '\n  '0.167*\"great\"'),\n (1,\n  '0.167*\"boxer\" + 0.167*\"win\" + 0.167*\"Mike\" + 0.167*\"belt\" + 0.167*\"Tyson\" + '\n  '0.167*\"great\"'),\n (2,\n  '0.167*\"win\" + 0.167*\"boxer\" + 0.167*\"Mike\" + 0.167*\"belt\" + 0.167*\"great\" + '\n  '0.167*\"Tyson\"'),\n (3,\n  '0.167*\"win\" + 0.167*\"boxer\" + 0.167*\"Mike\" + 0.167*\"Tyson\" + 0.167*\"belt\" + '\n  '0.167*\"great\"'),\n (4,\n  '0.167*\"win\" + 0.167*\"belt\" + 0.167*\"boxer\" + 0.167*\"Mike\" + 0.167*\"great\" + '\n  '0.167*\"Tyson\"'),\n (5,\n  '0.167*\"boxer\" + 0.167*\"Mike\" + 0.167*\"belt\" + 0.167*\"win\" + 0.167*\"great\" + '\n  '0.167*\"Tyson\"'),\n (6,\n  '0.276*\"boxer\" + 0.145*\"Mike\" + 0.145*\"Tyson\" + 0.145*\"great\" + 0.145*\"belt\" '\n  '+ 0.145*\"win\"'),\n (7,\n  '0.167*\"boxer\" + 0.167*\"win\" + 0.167*\"Mike\" + 0.167*\"belt\" + 0.167*\"Tyson\" + '\n  '0.167*\"great\"'),\n (8,\n  '0.167*\"boxer\" + 0.167*\"Mike\" + 0.167*\"Tyson\" + 0.167*\"great\" + 0.167*\"belt\" '\n  '+ 0.167*\"win\"'),\n (9,\n  '0.167*\"belt\" + 0.167*\"boxer\" + 0.167*\"win\" + 0.167*\"Mike\" + 0.167*\"Tyson\" + '\n  '0.167*\"great\"')]\n"
     ]
    }
   ],
   "source": [
    "topics = lda_model.print_topics(num_words=10)\n",
    "pprint(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}